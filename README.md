# Deep-learning-HW1

The codes are based on tensorflow 1.14. Python 3.6.  They can be run in the jupyter-notebook.

## Deep a function
### Simulate a function: HW1-1 Simulated function_DNN.ipynb
### Train on actual task: HW1-1 MINST_DNN.ipynb
### Bonus: HW1-1 MINST_CNN.ipynb

## Optimization
### Visualize the optimization process: HW1-2 Weights PCA.ipynb
### Observe gradient norm during training: HW1-2 Optimization_gradient_function.ipynb
### What happens when gradient is almost zero: HW1-2 Optimization_gradient_function.ipynb

## Generalizaton
### Can network fit random labels?: HW1-3_MINST_DNN_Shuffled_label.ipynb
### Number of parameters v.s. Generalization: HW1-3_MINST_DNN_Ten_Models.ipynb
### Flatness v.s. Generalization - part1: HW1-3_MINST_DNN_different batch.ipynb
### Flatness v.s. Generalization - part2: HW1-3_MINST_DNN_sensitivity.ipynb

